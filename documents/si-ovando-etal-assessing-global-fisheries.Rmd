---
title: "Supplemental Information: Global Assessments of Fishery Status need Better Data more than Better Models"
author: 
  - Daniel Ovando
  - Ray Hilborn
  - Cole Monnahan
  - Merrill Rudd
  - Rishi Sharma
  - James Thorson
  - Yannick Rousseau
  - Yimin Ye
date: "`r Sys.Date()`"
bibliography: ["../references.bib"]
csl: nature.csl
output: 
  bookdown::pdf_document2: default
  bookdown::word_document2:
    reference_docx: word-template.docx
params:
  results_name: ["v0.5"]
  min_years_catch: [25]
abstract: |
  Assessments of the global state of fisheries play an important role in shaping the public narrative around ocean health, motivating future directions of research and funding, formulating evidence-based policy and tracking the implementation of the United Nations Sustainable Development Goals. While we have reliable estimates of stock status for fisheries accounting for 50% of global catch, our knowledge of the state of the remaining 50%, the worlds 'unassessed' fisheries, is poor. Numerous high-profile publications featuring a range of statistical methods have produced estimates of the global status of these unassessed fisheries, but limited quantity and quality of data along with methdological differences have produced counterintuitive and conflicting results. This is especially true in areas such as Southeast Asia and Africa which are also regions that have high dependence on fishery resources. How can we effectively estimate the state of global fishery sustainability and track progress towards the Sustainable Development Goals targets? We developed a flexible assessment model to assess the value of different kinds, quantities, and quality of data in improving estimates of fishery stock status. We then explore avenues for obtaining potentially impactful data, including through the use of local expert opinion through Fisheries Management Index scores, and increasingly available but historically underutilized data such as trawl footprints and effort data. These data are then used to illustrate how different types of information paint starkly different pictures of the state of fisheries around the world, and to identify priority data types for future collection. Our results provide further evidence that catch data by themselves cannot provide reliable estimates of fishery health. Our ability to obtain reliable estimates of stock status for the world's unassessed fisheries depends on prioritizing the collection of key data at a global scale, not on the development of new modeling methods alone.
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, dpi = 600,
                      cache = FALSE, fig.width = 6, fig.asp = .75, dev = "png")
library(tidyverse)
library(viridis)
library(sf)
library(hrbrthemes)
library(extrafont)
library(scales)
library(patchwork)
library(rstanarm)
library(bayesplot)
library(here)
library(sraplus)
extrafont::loadfonts()

functions <- list.files(here::here("functions"))

purrr::walk(functions, ~ source(here::here("functions", .x)))

results_name <- params$results_name

results_path <- here("results",results_name)

min_years_catch <- params$min_years_catch

prepare_sofia_data(lookup_fmi_names = FALSE)

ram_comp_data <- ram_data %>%
  mutate(has_things = !(is.na(catch) | catch == 0)) %>%
  filter(has_things) %>%
  group_by(stockid) %>%
  mutate(delta_year = as.integer(year - lag(year))) %>%
  mutate(delta_year = case_when(year == min(year) ~ as.integer(1),
                                TRUE ~ delta_year)) %>%
  mutate(missing_gaps = any(delta_year > 1)) %>%
  filter(missing_gaps == FALSE) %>%
  group_by(stockid) %>%
  mutate(n = length(catch)) %>%
  filter(n >= min_years_catch) %>%
  ungroup()

ram_comp_data <- ram_comp_data %>%
  select(
    year,
    stockid,
    country,
    primary_FAOarea,
    scientificname,
    commonname,
    year,
    catch,
    b_v_bmsy,
    u_v_umsy,
    total_biomass
  ) %>%
  rename(scientific_name = scientificname) %>% 
  mutate(fao_area_code = as.numeric(primary_FAOarea)) %>% 
  left_join(fao_species, by = "scientific_name")


assess_ram_fits <- read_rds(path = file.path(results_path,"assess_ram_fits.rds"))

ram_v_sraplus_area <- read_rds(path = file.path(results_path,"ram_v_sraplus_area.rds"))

ram_comp_data <- read_rds(path = file.path(results_path,"ram_comp_data.rds"))

fao_sraplus_comp_data <- read_rds(path = file.path(results_path, "fao_sraplus_comp_data.rds"))


load(file = file.path(results_path, "paper_plots.Rdata"))

load(file = file.path(results_path, "voi_fits.RData"))



pub_theme <- theme_ipsum(base_size = 10,
                         axis_text_size = 10,
                         axis_title_size = 12) +
    theme(panel.spacing = unit(.5,"lines"))

theme_set(pub_theme)


```

# Supplemental Information

## Population Model

The core of our model is a Pella-Tomlinson [@pella1969] production model  in the manner of @winker2018. While models of these kinds abstract away many important details of fish biology and fleet behavior, they are the highest resolution model that the potential data evaluated here will support. 

```{r si-prior-tab}
cd <- data_frame(Parameter = "Carrying Capacity", Abbreviation = "K", `Default Prior` = "$logn(10\\times{max(catch)},4)$") %>%
  rbind(c("Growth rate", "r", "$logn(r_{fishlife}, \\sigma_{r,fishlife})$")) %>%
  rbind(c("Shape parameter", "m", "$logn(1.01, 0.25)$")) %>%
  rbind(c("Catchability", "q", "$logn(1e^{-3}, 0.3)$")) %>%
  rbind(c("Observation Error", "$\\sigma_{obs}$", "$logn(.05,2)$")) %>%
  rbind(c("Process Error", "$\\sigma_{proc}$", "$logn(.05,0.5)$")) %>% 
  rbind(c("Tech Creep", "$creep$", "$logn(0.025,0.2)$"))

 
knitr::kable(
  cd,
  "latex",
  align = "l",
  booktabs = TRUE,
  escape = F,
  row.names = F,
  caption = "Potential parameters included in sraplus, abbreviations, and default priors"
) %>%
  kableExtra::kable_styling(full_width = T, font_size = 9) %>%
  kableExtra::row_spec(0, bold = T)
```


The population growth equation is  

\begin{equation}
  f(x)=\begin{cases}
      B_{t + 1} = \left(B_{t} + B_{t}\frac{r}{m - 1}\left(1 - \left(\frac{B_t}{K}\right)^{m- 1}\right) - \hat{c_t}\right)p_t
, & \text{if $B_t>0.25 \times K$}.\\
     B_{t + 1} = \left(B_{t} + \frac{B_{t}}{0.25 \times K}\left(B_{t}\frac{r}{m - 1}\left(1 - \left(\frac{B_t}{K}\right)^{m- 1}\right) - \hat{c_t}\right)\right)p_t, & \text{otherwise}.
  \end{cases}
  (\#eq:sihockey)
\end{equation}

Where $B_t$ is biomass at time *t*, *r* is the intrinsic growth rate, *m* is the scaling parameter that allows for the ratio of Bmsy/K to shift (when *m* is 2 the Bmsy / K = 0.5 and the model acts as a Schaefer model, lower values of m shift the production function left, higher values right), *c* is catch, and *p* is process error. The Pella-Tomlinson model does have an issue where growth rates can become unrealistically large when the population reaches low sizes. We deal with this problem by following the methods described in @winker2018. to reduce the production of the population when it falls below a threshold of 25% of carrying capacity. Incorporation of process errors is useful for two reasons: (1) when you have an abundance index, process errors can reduce bias arising from lack of fit in a deterministic SRA whenever dynamics are poorly explained by catch-history alone, and (2) with or without an abundance index (or other info), the stochastic portion is necessary to get good uncertainty intervals (i.e., with close to nominal coverage, see @thorson2018). 

<!-- When the SIR method is used we use the supplied catch history. However, for the maximum likelihood approach, convergence is greatly facilitated by estimating catches ($\hat{c_t}$) rather than using the observed catches directly. $\hat{c_t}$ is given as -->

<!-- \begin{equation} -->
<!--   \hat{c_t} = u_{t}B_{t} -->
<!--   (\#eq:chat) -->
<!-- \end{equation} -->


<!-- where $u_t$ is estimated through -->

<!-- \begin{equation} -->
<!--   u_{t} = \frac{1}{1 + e^{-u^*_t}} -->
<!--   (\#eq:invf) -->
<!-- \end{equation} -->

<!-- Equation \@ref(eq:invf) ensures that $u_t$ is between 0 and 1, while the model estimates $u^*_t$ to improve performance.  -->

<!-- $\hat{c_t}$ is then fit to the observed catches through the likelihood -->

<!-- \begin{equation} -->
<!--   log\left(c_t\right) \sim normal\left(log\left(\hat{c_t}\right),0.05\right) -->
<!--   (\#eq:catch) -->
<!-- \end{equation} -->

<!-- Where we assume an observation coefficient of variation of 5\%. We estimate catch rather than directly using the observed catch in order to improve model performance. As priors on depletion, or indices of abundance, pull the estimated population closer to zero, there is a greater and greater chance that the observed catch history will result in a collapse of the population (catch in time *t* being greater than biomass in time *t*); It is much harder to find parameters that produce a final depletion of 1\% of carrying capacity while not crashing the population prior to then given a catch history, than it is to produce a final depletion of 99\% of carrying capacity. We can impose restrictions and penalties on population crashes, but this distorts the gradients of the model, causing the model to often become stuck once a few crashes have occurred. Since $u_t$ is constrained between 0 and 1, $\hat{c_t}$ will always be "viable", and by then fitting to the observed catches, the model has an easier time finding parameters that produce a near match to the observed catches while not crashing the population. As of now none of the data used in this exercise have missing years of catch data, but the model can handle those cases in the future due to the random walk priors imposed on $u_t$ detailed below.  -->

<!-- In order to help separate out observation error in the catch and process error in the population model, when catch is being estimated we assume that fishing mortality $u_t$ follows a random walk on average, assigning each $u_t$ a prior of -->

<!-- \begin{equation} -->
<!--   log(u_t) \sim normal(log(u_{t - 1}),0.1)) -->
<!--   (\#eq:rwalk) -->
<!-- \end{equation} -->


We  allow for process error (in the manner of the stochastic stock reduction analysis error suggested by @walters2006). This allows the population dynamics to deviate from the exact values given by the Pella-Tomlinson operating model, while still conforming to the assumptions of this model on average. Process error *p* is assumed to be log-normally distributed, such that 

\begin{equation}
  p_t \sim e^{normal\left(-\sigma_{proc}^2/2,\sigma_{proc}\right)}
  (\#eq:siprocerror)
\end{equation}


All of our estimates are Bayesian in nature. We can break the use of our model into two distinct categories: with data and without. By "data", we refer to measurements which are confronted with model estimates within a likelihood function. In our context, these include fishery-independent survey data, or a CPUE index. When there are no data, the model amounts to filtering priors through the model (the combination of the Pella-Tomlinson operating model and the catches for the stock in question, along with any fixed parameters). Under this mode, the model is essentially a stock-reduction analysis model, in the manner of @walters2006, in which we ask, which combinations of prior probability distributions of parameters do not crash the population, given the constraints of the model. 

We use a Bayesian methods as it suits the philosophy of our analysis: incorporating prior knowledge about stock status drawn from expert opinion or other similar stocks, and where possible confronting those priors with data. However, this process does introduce a problem, in that it a) certain parts of the prior probability distribution have zero support in the data (e.g. carrying capacity's so small that they crash the population) and b) they create priors on both the *input* (e.g. growth rates and carrying capacity) and *outputs* (recent stock status), effectively putting two prior distributions for the same outcome, a problem termed Borel's Paradox XX punt citation. This may seem like an academic concern, and indeed in our experience when the data are sufficiently informative the Bayesian version of our model subject to Borel's paradox produces effectively identical results to those produce by the same model fit by maximum likelihood. However, particularly for the data-less version of our model, Borel's Paradox poses a particular problem. 

Consider a production model with two parameters, a growth rate *r* and a carrying capacity *k*. Once we specify prior distributions on r and k, and then apply these distributions to our model (the shape of the production function along with the catch histories), we have implicitly provided a prior on the status of the stock in all time periods, since each unique combination of r and k together with the model produces a deterministic stock status in each time step. Suppose then we wish to also provide a prior on stock status in the most recent year, for example to reflect local expert opinion based on sound reasoning. We have now placed two priors on stock status in the most recent year, one implicit and one explicit, violating Borel's Paradox. The net effect of this is that once these priors are run through the model, through for example a sample-importance-resampling (SIR) algorithm as generally applied in stock-reduction analysis, our post-model-pre-data "estimate" of stock status will appear to have "updated". This creates the illusion that we have learned something about stock status, when in fact we have simply used our model to reveal our true implied prior on stock status. This is technically incorrect, since we do not have a likelihood in the model at this point, and as such per Bayes Theorem have not updated our prior. Of more practical importance, this "implied prior" will always reflect a more optimistic stock status then our original prior on stock status, due to the constraints of the population model. 

The SRA algorithm works in two steps. First, the algorithm rejects any draws that resulted in the collapse of the population (biomass less than catch in a given timestep). From there a standard SRA would sample from the priors in proportion to the stated prior on recent stock status, e.g. if the bulk of the prior on terminal stock status was concentrated at 50% of *k*, combinations of *r* and *k* that produce terminal stock status near 50% of *k* are sampled proportionally more frequently. However, lower values of terminal stock status have fewer candidate values of *r* and *k*, since it becomes harder and harder to find viable pairs that come close to but do not crash the population at any time step. Conversely, in the absence of constraints higher values of stock status have infinite combinations of plausible *r* and *k* combinations: since under this model the population cannot be greater than carrying capacity, as for example *k* approaches infinity terminal stock status asymptotes at close to 100% of *k*. The net result of this is that even though individual combinations of *r* and *k* that produce higher stock status than the mean of the prior on recent stock status individually have lower probability of being sampled, there are many more opportunities for the lower-probability events that produce higher stock status to be sampled. As a result, the post-model-pre-data prior on terminal depletion will always be higher under this method than the supplied prior on stock status. 

To resolve this, @puntXX (either cite that "on assessment of the bering-chukchi-beufort sea stock of bowhead whales paper that I can't find a date for, or see if he has a new paper on thisXX) suggests a "backwards" fitting approach, in which the model estimates growth parameters and a terminal depletion, and given the model backs out a value of *k* that satisfies the priors (conditional on an assumption initial stock status). We do not adopt this approach here due to computational issues. Adopting the "backwards" approach requires numerically searching for the plausible *k* during each iteration of the model, which is time consuming. In addition, the backwards approach is not without its own complications, for example the requirement of a tolerance around the value of terminal depletion produced by a given *k* and the actual value selected by the model in that time step. This discrepancy can occur when a given terminal stock status is not technically possible given the model; to give an extreme example under this model no value of *k* can produce a terminal stock status of 0 if catches are 0 for the entire history of the fishery (assuming *k* > 0). 

We present a novel approximation to this problem here. Our solution amounts to a two-step SIR algorithm. We first run the standard SIR algorithm as described above. We then break the resulting draws into bins based on terminal stock status, and calculate the mean sampling probability of each bin.

$$p(bin_i) = \frac{1}{N_i}\sum_{n = 1}^{N_i}{p(b_{n,i})}$$

We then divide the sampling probability of of bin *i* evenly among each of the draws within that bin *n*

$$p(n_i) = \frac{p(bin_i)}{N_i}$$

And we then perform a second SIR algorithm but now sampling each observation $n_i$ in proportion to $p(n_i)$. The net result of this is a post-model-pre-data distribution of parameters *r* and *k*  that produce a distribution of recent stock status that roughly matches the supplied prior on recent stock status. In effect, this answers the question "given the model, what combinations of parameters produce my prior on recent stock status". 

XX insert figure if there's space illustrating this. 

<!-- @raftery1993 -->



```{r fao-acc-map, fig.cap = "Mean classification accuracy (assignment to FAO stock status category) by FAO statistical area arising from different data sources. ram-data refers to catch and abundance index drawn from RAM. CPUE refers to an index of abundance based on the Rousseau effort data. Cpue-plus uses CPUE along with fisheries management index (FMI) and/or swept area ratio data. The FAO Report 569 is our reference status estimate provided by the FAO. ram_u_umsy assumes all fisheries in the region share a common U/Umsy series with formally assessed fisheries in the region. fmi uses fmi scores to develop a prior on recent fishing mortality rates, sar does the same but based on swept area ratio"}

fao_acc_map_plot

```



```{r}

rand_fits <- fao_sraplus_comp_data %>% 
  filter(data == "catch_only") %>% 
  mutate(bin = sample(c("over", "fully", "under"), n(), replace = TRUE),
         data = "guess") %>% 
  mutate(correct = bin == fao_bin)

fao_sraplus_acc <- fao_sraplus_comp_data %>% 
  bind_rows(rand_fits) %>% 
  group_by(data) %>% 
  summarise(accuracy = mean(correct, na.rm = TRUE)) %>% 
  arrange(desc(accuracy))

knitr::kable(fao_sraplus_acc,
             caption = "Accuracy of different data sources in classifying stocks reported in the FAO SOFIA report to FAO status categories. Guess corresponds to randomly assigning a stock to one of the FAO status categories.",
             format = "pipe", 
             digits = 2)

```

```{r}

rand_fits <- fao_sraplus_comp_data %>% 
  filter(data == "catch_only") %>% 
  mutate(bin = sample(c("over", "fully", "under"), n(), replace = TRUE),
         data = "guess") %>% 
  mutate(correct = bin == fao_bin)

fao_sraplus_area_acc <- fao_sraplus_comp_data %>% 
  bind_rows(rand_fits) %>% 
  group_by(data, fao_area_code) %>% 
  summarise(accuracy = mean(correct, na.rm = TRUE)) %>% 
  arrange(desc(accuracy)) %>% 
  pivot_wider(names_from = data, values_from = accuracy) %>% 
  arrange(fao_area_code)

knitr::kable(fao_sraplus_area_acc,
             caption = "Accuracy of different data sources in classifying stocks reported in the FAO SOFIA report to FAO status categories by FAO major statistical area. Guess corresponds to randomly assigning a stock to one of the FAO status categories.",
             format = "pipe", 
             digits = 2)

```




## SAR

## FMI

```{r fmi-fit, fig.cap = "Observed (x-axis) vs posterior predictive (y-axis) F/F~MSY~ for regression of fisheries management index (FMI) on F/F~MSY~"}

fit <- sraplus::fmi_models$fit[sraplus::fmi_models$metric == "mean_uumsy"][[1]]

fit_data <- fit$data

pp <- posterior_predict(fit, draws = 1000)

r2_plot <- tibble(r2 = bayes_R2(fit)) %>% 
  ggplot(aes(r2)) + 
  geom_histogram() + 
  labs(subtitle = "B) Bayesian R2")

pi_plot <- ppc_intervals(y = fit_data$log_value, yrep = pp, x = fit_data$log_value) + 
  scale_x_continuous(name = "Observed Log F/Fmsy") + 
  scale_y_continuous(name = "Posterior Predictive F/Fmsy") +
  labs(subtitle = "A) Observed vs. Posterior Predictive") +
 pub_theme

pi_plot + r2_plot
```



```{r sar-fit, fig.cap = "Observed (x-axis) vs posterior predictive (y-axis) F/F~MSY~ for regression of swept area ratio (SAR) on F/F~MSY~"}
fit <- sraplus::sar_models$fit[sraplus::sar_models$metric == "mean_uumsy"][[1]]

fit_data <- fit$data

pp <- posterior_predict(fit, draws = 1000)

r2_plot <- tibble(r2 = bayes_R2(fit)) %>% 
  ggplot(aes(r2)) + 
  geom_histogram() + 
  labs(subtitle = "B) Bayesian R2")

pi_plot <- ppc_intervals(y = fit_data$log_value, yrep = pp, x = fit_data$log_value) + 
  scale_x_continuous(name = "Observed Log F/Fmsy") + 
  scale_y_continuous(name = "Posterior Predictive F/Fmsy") +
  labs(subtitle = "A) Observed vs. Posterior Predictive") +
 pub_theme

pi_plot + r2_plot

```

## Catch

```{r catch-reg,fig.cap = "Observed (x-axis) vs posterior predictive (y-axis) B/B~MSY~ for regression of catch on B/B~MSY~"}

fit <- sraplus::catch_b_model

fit_data <- fit$data %>% 
  group_by(stockid) %>% 
  filter(year == max(year))

pp <- posterior_predict(fit, draws = 1000, newdata = fit_data)

r2_plot <- tibble(r2 = bayes_R2(fit)) %>% 
  ggplot(aes(r2)) + 
  geom_histogram() + 
  labs(subtitle = "B) Bayesian R2")

pi_plot <- ppc_scatter_avg(y = fit_data$log_value, yrep = pp) + 
  coord_flip() +
  scale_y_continuous(name = "Observed Log B/Bmsy") + 
  scale_x_continuous(name = "Posterior Predictive B/Bmsy") +
  labs(subtitle = "A) Observed vs. Posterior Predictive") +
 pub_theme


pi_plot + r2_plot
```

